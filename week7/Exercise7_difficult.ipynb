{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning - Exercise 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is two-fold:\n",
    "\n",
    "- to understand and become familiar with the Metropolis-Hastings algorithm\n",
    "- and to learn to use posterior samples to compute predictions and posterior quantities of interest.\n",
    "\n",
    "The exercise is divided into three parts\n",
    "\n",
    "- Part 1: The Metropolis-Hastings algorithm for a 1D problem\n",
    "- Part 2: The Metropolis-Hastings algorithm for a 2D problem\n",
    "- Part 3: Metropolis-Hastings for a Bayesian Poisson regression model\n",
    "\n",
    "In the first two parts, we will study the properties of the Metropolis-Hastings algorithm as well as its tuning parameters. In the part 3, we will use the algorithm to compute the posterior predictive distribution for a fully Bayesian Poisson regression model.\n",
    "\n",
    "Note that to run this notebook, you'll also need the package called **tqdm** for plotting progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as snb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# distributions\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# for plotting and visualization\n",
    "from week7.exercise7 import plot_predictions\n",
    "from week7.exercise7 import eval_density_grid\n",
    "\n",
    "# style stuff\n",
    "snb.set_style('darkgrid')\n",
    "snb.set(font_scale=1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We will need the following function for evaluating densities and log densities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normal distribution\n",
    "log_npdf = lambda x, m, v: -0.5*(x-m)**2/(v) - 0.5*np.log(2*np.pi*v)\n",
    "npdf = lambda x, m, v: np.exp(log_npdf(x, m, v))\n",
    "\n",
    "# Half-normal distribution\n",
    "log_half_npdf = lambda x, m, v: np.log(2) -0.5*(x-m)**2/(v) - 0.5*np.log(2*np.pi*v)\n",
    "half_npdf = lambda x, m, v: np.exp(log_half_npdf(x, m, v))\n",
    "\n",
    "# Laplace distributions\n",
    "log_laplace = lambda x: np.log(0.5) - np.abs(x)\n",
    "laplace = lambda x: np.exp(log_laplace(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1: The Metropolis-Hastings algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of sampling-based methods is that we can compute any posterior quantity of interest if we can obtain a set of samples from the posterior. If $\\mathbf{\\theta}^{1}, \\mathbf{\\theta}^{2}, \\dots, \\mathbf{\\theta}^{S} \\sim p(\\mathbf{\\theta}|\\mathbf{t})$, then we can approximate any posterior expectation using a **Monte Carlo estimate** as follows\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{\\mathbf{\\theta}|\\mathbf{t}}\\left[f(\\mathbf{\\theta})\\right] \\approx \\frac{1}{S}\\sum_{i=1}^S f(\\mathbf{\\theta}^i) \n",
    "\\end{align}\n",
    "\n",
    "In practice, it turns out to be very difficult to obtain **i.i.d** samples from general posterior distributions.  However, **Markov Chain Monte Carlo methods (MCMC)** provides a method for generating samples from almost any reasonable distribution, but the price is that the samples are often highly correlated, thus reducing the effective sample size.\n",
    "\n",
    "The core idea of MCMC methods is to construct a Markov chain such that its stationary distribution will be equal to the distribution we want to sample from. Informally, we are creating a type of random walk that explores the parameter space of the target distribution in way where regions with high density are visited more frequently than areas with low density.\n",
    "\n",
    "Today, we will work with the algorithm called the **Metropolis-Hastings** algorithm. Suppose our goal is to generate samples from a distribution over $\\mathbf{\\theta}$ (e.g. a prior distribution $p(\\mathbf{\\theta})$ or a posterior distribution $p(\\mathbf{\\theta}|\\mathbf{t})$). Recall, we only need to be able to evaluate the target density up to constant. That is, if $p(\\mathbf{\\theta}) = \\frac{1}{Z}\\tilde{p}(\\mathbf{\\theta})$, then it is sufficient to be able to evaluate $\\tilde{p}(\\mathbf{\\theta})$.\n",
    "\n",
    "Every iteration of the MH algorithm proceeds as follows. Assume that the sample at iteration $k-1$ is $\\theta^{k-1}$ and we want to compute the sample for iteration $k$. Then, the first step is to generate a new **candidate sample** $\\theta^{\\star}$ using a **proposal distribution** $q(\\theta^{\\star}|\\theta^{k-1})$. The proposal distribution is chosen in a way such that it is easy to sample from it.\n",
    "\n",
    "In this exercise, we will use Gaussian distributions as proposal distributions:\n",
    "\n",
    "\\begin{align*}\n",
    "    q(\\theta^{\\star}|\\theta^{k-1}) = \\mathcal{N}(\\theta^{\\star}|\\theta^{k-1}, \\tau)\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta^{\\star} = \\theta^{k-1} + e_n, \\quad\\quad e_n \\sim \\mathcal{N}(0, \\tau).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "That is, at iteration $k$, we generate the new candidate sample $\\mathbf{\\theta}^{\\star}$ by taking sample for iteration $k-1$, i.e. $\\mathbf{\\theta}_{k-1}$, and adding a small amount of noise to it. The parameter $\\tau$ is the variance of the proposal distribution and it controls how much noise we add to $\\mathbf{\\theta}^k$ when generating the candidate sample $\\mathbf{\\theta}^{\\star}$. In other words, the parameter $\\tau$ controls the **step-size** of the algorithm.\n",
    " \n",
    "Once the candidate sample $\\mathbf{\\theta}^{\\star}$ has been generated, the next step is to calculate the **acceptance probability** $A_k$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "A_k = \\min(1, r_k), \\quad\\quad\\quad\\text{where}\\quad r_k = \\frac{p(\\mathbf{\\theta}^{\\star})  q(\\mathbf{\\theta}^{k-1}|\\mathbf{\\theta}^{\\star}) }{p(\\mathbf{\\theta}^{k-1}) q(\\mathbf{\\theta}^{\\star}|\\mathbf{\\theta}^{k-1})} \\tag{1}.\n",
    "\\end{align*}\n",
    "\n",
    "We now accept the new candidate sample $\\mathbf{\\theta}^{\\star}$ with probability $A_k$. We can implement this by generating a sample $u_k \\sim \\mathcal{U}(0, 1)$ from a uniform distribution in the interval $(0, 1)$. If $u_k < A_k$, we accept the candidate $\\mathbf{\\theta}^{\\star}$ and set $\\mathbf{\\theta}^{k} = \\mathbf{\\theta}^{\\star}$. Otherwise, we reject new candidate and set $\\mathbf{\\theta}^{k} = \\mathbf{\\theta}^{k-1}$. That is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\theta}_k = \\begin{cases}\\mathbf{\\theta}^{\\star} & \\text{if}\\quad u_k < A_k\\\\ \\tag{2}\n",
    "\\mathbf{\\theta}^{k-1} & \\text{otherwise}\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The Metropolis-Hastings algorithm proceeds like this until we have reached a predefined number of iterations. Recall, when working with probabilities on computers, it is numerically more stable to do as many calculations in the log-domain as possible and exponentiate as late as possible. Therefore, we should calculate $\\log(r_k)$ and calculate $A_k$ as\n",
    "\n",
    "\\begin{align*}\n",
    "A_k = \\min(1, \\exp[\\log(r_k)]) \\tag{3}\n",
    "\\end{align*}\n",
    "\n",
    "The implementation of MH algorithm below is almost complete. Your first task is the understand the code and implement the missing pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metropolis(log_joint, num_params, tau, num_iter, theta_init=None, seed=None):    \n",
    "    \"\"\" Runs a Metropolis-Hastings sampler \n",
    "    \n",
    "        Arguments:\n",
    "        log_joint:          function for evaluating the log joint distribution\n",
    "        num_params:         number of parameters of the joint distribution (integer)\n",
    "        tau:                variance of Gaussian proposal distribution (positive real)\n",
    "        num_iter:           number of iterations (integer)\n",
    "        theta_init:         vector of initial parameters (np.array with shape (num_params) or None)        \n",
    "        seed:               seed (integer or None)\n",
    "\n",
    "        returns\n",
    "        thetas              np.array with MCMC samples (np.array with shape (num_iter+1, num_params))\n",
    "    \"\"\" \n",
    "    \n",
    "    # prevent progress bar from causing issues\n",
    "    if hasattr(tqdm,'_instances'):\n",
    "        tqdm._instances.clear()\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if theta_init is None:\n",
    "        theta_init = np.zeros((num_params))\n",
    "    \n",
    "    # prepare lists \n",
    "    thetas = [theta_init]\n",
    "    accepts = []\n",
    "    log_p_theta = log_joint(theta_init)\n",
    "    \n",
    "    for k in tqdm(range(num_iter)):\n",
    "        # implement the MH loop here\n",
    "\n",
    "        \n",
    "        \n",
    "    print('Acceptance ratio: %3.2f' % np.mean(accepts))\n",
    "        \n",
    "    thetas = np.stack(thetas)\n",
    "    return thetas\n",
    "\n",
    "\n",
    "# sanity check: estimate the mean and variance of a N(x|1,3) Gaussian distribution\n",
    "thetas = metropolis(lambda x: log_npdf(x, 1., 3.), 1, 2., 10000, theta_init=np.array([0]), seed=0)\n",
    "\n",
    "# compute mean and variance and relative errors\n",
    "mean_thetas, var_thetas = np.mean(thetas), np.var(thetas)\n",
    "rel_err_mean, rel_err_var = (mean_thetas - 1.)/1., (var_thetas - 3.)/3.\n",
    "\n",
    "print('\\nSanity check:')\n",
    "print(f'Mean:\\t{mean_thetas:3.2f} (rel. err: {rel_err_mean:3.2f})')\n",
    "print(f'Var:\\t{var_thetas:3.2f} (rel. err: {rel_err_var:3.2f})')\n",
    "\n",
    "if np.abs(rel_err_mean) < 0.05 and np.abs(rel_err_var) < 0.05:\n",
    "    print('The relative errors for the first two central moments are less than 5%. Everything looks ok.')\n",
    "else:\n",
    "    print('The relative errors for the first two central moments are larger than 5%. Something is likely wrong. Check your implementation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1.1) Complete the implementation of the Metropolis-Hastings algorithm above. See eq. (1), (2) and (3) above. Don't hesitate to ask for help if you are stuck in this question.\n",
    "\n",
    "\n",
    "1.2) Why is it not necessary to include the proposal probabilities $q(\\theta^{k-1}|\\theta^{\\star})$ and $q(\\theta^{\\star}|\\theta^{k-1})$ when computing the acceptance probability for Gaussian proposals?\n",
    "\n",
    "\n",
    "1.3) What is the resulting acceptance probability $A_k$ when the candidate sample $\\theta^{\\star}$ has higher density value than the previous sample $\\theta^{k-1}$?\n",
    "\n",
    "\n",
    "1.4) Use eq. (1) one above to argue that we don't need to be able to evaluate the joint density of the target distribution, but only the log joint density of up to a constant. That is, we only need to be evalute $\\tilde{p}$. Why is this important in Bayesian modelling?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the sampler** \n",
    "\n",
    "The code below runs the Metropolis-Hastings algorithm for the target distribution specified through the **log_joint** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify number of parameters in the target distribution\n",
    "num_params = 1\n",
    "\n",
    "# specify the target distribution\n",
    "log_joint = lambda x: log_npdf(x, 0, 1)\n",
    "#log_joint = lambda x: log_laplace(x)\n",
    "#log_joint = lambda x: np.log(0.5*laplace(x+2) + 0.5*laplace(x-2))\n",
    "\n",
    "# specify the parameters of the MH algorithm\n",
    "num_iterations = 10000\n",
    "warm_up = int(0.5*num_iterations)\n",
    "tau = 0.5\n",
    "\n",
    "# run sampler\n",
    "thetas = metropolis(log_joint, num_params, tau, num_iterations, theta_init=np.array([(-10)]), seed=0)\n",
    "\n",
    "# plot resutls\n",
    "xs = np.linspace(-5, 5, 1000)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "axes[0].plot(thetas)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Parameter $\\\\theta$')\n",
    "axes[0].set_title('Trace of parameter $\\\\theta$', fontweight='bold')\n",
    "\n",
    "axes[1].hist(thetas, 20, density=True);\n",
    "axes[1].plot(xs, np.exp(log_joint(xs)), linewidth=3)\n",
    "axes[1].set_xlabel('Parameter $\\\\theta$')\n",
    "axes[1].set_title('Histogram of all samples', fontweight='bold')\n",
    "\n",
    "axes[2].hist(thetas[warm_up:], 20, density=True);\n",
    "axes[2].plot(xs, np.exp(log_joint(xs)), linewidth=3)\n",
    "axes[2].set_xlabel('Parameter $\\\\theta$');\n",
    "axes[2].set_title('Histogram of all samples after warm-up', fontweight='bold');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1.5) Explain what you see in the three panels above.\n",
    "\n",
    "\n",
    "1.6) Why do we need to discard the first proportion of samples (the warm-up phase)?\n",
    "\n",
    "\n",
    "1.7) What happens to the acceptance ratio if you make the variance of the proposal distribution smaller, e.g. $\\tau = 0.1$? How well does the resulting histogram of samples match the true target distribution?\n",
    "\n",
    "\n",
    "1.8) What happens of you make the proposal variance larger, e.g. $\\tau = 10$ or $\\tau=500$? How well does the resulting histogram of samples match the true target distribution?\n",
    "\n",
    "1.9) Using $\\tau = 0.5$ and keeping warm up at 50%, how many iterations do you need before distribution of the samples resembles the target distribution (based on a visual comparison of the target density and the histogram of the samples)?\n",
    "\n",
    "1.10) Experiment with the two other target functions specified above.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The metropolis-Hasting algorithm in 2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 2, we will apply MH algorithm to generate samples from a bivariate normal distribution with correlation $\\rho$. That is, the target density is\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{\\theta}|\\mu, \\Sigma),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{\\theta} \\in \\mathbb{R}^2$, $\\mathbf{\\mu} = \\begin{bmatrix}-4\\\\ 4\\end{bmatrix}$, and $\\mathbf{\\Sigma} = \\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}$.\n",
    "\n",
    "There are, of course, much more efficient ways to generate samples from multivarate normal distributions, but here we use the multivariate normal distribution to understand the properties of the MH algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify target density\n",
    "mu = np.array([-4, 4])                            # mean \n",
    "rho = 0.95                                        # correlation coef.\n",
    "S = np.array([[1, rho], [rho, 1]])                # covariance matrix\n",
    "log_joint = lambda x: mvn.logpdf(x, mu, S)        # target density\n",
    "\n",
    "# specify number of parameters of the target density\n",
    "num_params = 2\n",
    "\n",
    "# sampler settings\n",
    "num_iterations = 10000\n",
    "warm_up = int(0.5*num_iterations)\n",
    "tau = 0.5\n",
    "\n",
    "# run sampler\n",
    "thetas = metropolis(log_joint, num_params, tau, num_iterations, seed=0)\n",
    "\n",
    "# discard samples from warm-up phase\n",
    "#thetas = thetas[warm_up:, :]\n",
    "\n",
    "# plot\n",
    "xs = np.linspace(-7.5, 7.5, 100)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes[0, 0].plot(*thetas.T, lw=0.5, alpha=0.7, zorder=1)\n",
    "axes[0, 0].scatter(*thetas.T, c=\"k\", s=3, alpha=0.5, zorder=2)\n",
    "axes[0, 0].set_xlabel('Parameter $\\\\theta_1$')\n",
    "axes[0, 0].set_ylabel('Parameter $\\\\theta_2$')\n",
    "axes[0, 0].set_title('All MCMC samples', fontweight='bold')\n",
    "\n",
    "\n",
    "x_grid, log_density_grid = eval_density_grid(log_joint)\n",
    "axes[1, 0].scatter(*thetas.T, c=\"k\", s=3, alpha=0.5, label='MCMC samples')\n",
    "axes[1, 0].contour(x_grid, x_grid, np.exp(log_density_grid), levels=15, linewidths=0.75, colors='g')\n",
    "axes[1, 0].set_xlabel('Parameter $\\\\theta_1$')\n",
    "axes[1, 0].set_ylabel('Parameter $\\\\theta_2$')\n",
    "axes[1, 0].set_title('Contours of target', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i, 1].plot(thetas[:, i])\n",
    "    axes[i, 1].set_title('Trace of parameters $\\\\theta_{%d}$' % (i+1), fontweight='bold')\n",
    "    axes[i, 1].set_xlabel('Iteration')\n",
    "    axes[i, 1].set_ylabel('Parameter $\\\\theta_{%d}$' % (i+1))\n",
    "    \n",
    "    axes[i, 2].hist(thetas[:, i], 20, density=True, label='Samples')\n",
    "    axes[i, 2].set_xlabel('Parameter $\\\\theta_{%d}$' % (i+1))\n",
    "    axes[i, 2].plot(xs, npdf(xs, mu[i], S[i,i]), linewidth=3, label='Exact')\n",
    "    axes[i, 2].set_title('Histogram of samples for $\\\\theta_{%d}$' % (i+1), fontweight='bold')\n",
    "    axes[i, 2].legend()\n",
    "    \n",
    "    print('Mean and variance of samples for theta%d: %3.2f, %3.2f' % (i+1, np.mean(thetas[:, i]), np.var(thetas[:, i])) )\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the code above to answer the following questions:\n",
    "\n",
    "**Questions**\n",
    "\n",
    "2.1) Experiment with the number of iterations. How many iterations do you need before the sample mean and variance are equal to the true values (up to, say, 5%-10% relative error)?\n",
    "\n",
    "2.2) Repeat the experiment with and without discarding the samples from the warm-up phase. What changes do you see?\n",
    "\n",
    "2.3) What happens to the acceptance ratio if you increase the correlation in the target density to, say, $\\rho = 0.7$, $\\rho=0.9$, $\\rho=0.95$ or $\\rho=0.99$? What happens to the accuracy of the moments?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: MCMC for Bayesian Poisson regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the Metropolis-Hastings algorithm to Bayesian Poisson regression (discussed in lecture 5). As a case study, we will use a small data set from a study on mortality (Broffiti, 1988), where the authors looked at the mortality rates as a function of age. \n",
    "\n",
    "Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Raw data\n",
    "###################################################\n",
    "\n",
    "# The input covariate is age\n",
    "age = np.arange(35, 65).astype(np.float64)\n",
    "\n",
    "# The output is the number of deaths in the study popular for each age\n",
    "deaths = np.array([3, 1, 3, 2, 2, 4, 4, 7, 5, 2, 8, 13, 8, 2, 7, 4, 7, 4, 4, 11, 11, 13, 12, 12, 19, 12, 16, 12, 6, 10]).astype(np.float64)\n",
    "\n",
    "# number of data points\n",
    "N = len(deaths)\n",
    "\n",
    "# Let's standardize the input covariate and prepare \n",
    "m_age = np.mean(age)\n",
    "s_age = np.std(age)\n",
    "standardize = lambda x: (x-m_age)/s_age\n",
    "\n",
    "# prep data\n",
    "X = standardize(age)\n",
    "X = X[:, None]\n",
    "t = age[:, None]\n",
    "\n",
    "# Define input points for prediction\n",
    "age_pred = np.linspace(25, 70, 100)\n",
    "Xp = standardize(age_pred)\n",
    "\n",
    "# Let's set-up the design matrix for the training data and the inputs for predictions\n",
    "design_matrix = lambda x: np.column_stack((np.ones(len(x)), x))\n",
    "X_train = design_matrix(X)\n",
    "X_pred = design_matrix(Xp)\n",
    "\n",
    "# Plotting\n",
    "def plot_data(ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "    ax.plot(age, deaths, 'k.', markersize=10)\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('Number of deaths')\n",
    "    \n",
    "    ax.set_xlim((25, 70))\n",
    "    ax.set_ylim((-2, 25))\n",
    "\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will a use **generalized linear model** to model this dataset. Since the target variable $t_n$ is **count data**, i.e., $t_n \\in \\left\\lbrace 0, 1, 2, \\dots \\right\\rbrace$, we will use a Poisson likelihood to model the data. If you don't recall what the Poisson distribution looks like, then the Wikipedia page is useful: https://en.wikipedia.org/wiki/Poisson_distribution. \n",
    "\n",
    "The likelihood of a single data point is given by\n",
    "\n",
    "\\begin{align*}\n",
    "t_n|\\mu_n \\sim \\text{Poisson}(\\mu_n),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mu_n > 0$ is the mean parameter for the Poisson distribution, i.e., $\\mathbb{E}_{t_n|\\mu_n}\\left[t_n\\right] = \\mu_n$.\n",
    "\n",
    "Since count data are non-negative, we will use the **log link function** \n",
    "\n",
    "\\begin{align*}\n",
    "\\log(\\mu_n) = y_n \\quad\\quad \\iff \\quad \\quad \\mu_n = \\exp(y_n),\n",
    "\\end{align*}\n",
    "\n",
    "where $y_n = y(\\mathbf{x}_n)$ is a linear model with parameters $\\mathbf{w} \\in \\mathbb{R}^2$\n",
    "\n",
    "\\begin{align*}\n",
    "y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}_n = w_0 + w_1\\cdot\\text{age}.\n",
    "\\end{align*}\n",
    "\n",
    "The model has two parameters: an intercept $w_0$ and a slope $w_1$. To perform Bayesian inference,  we will impose a Gaussian prior distribution on both for regularization\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}|\\kappa^2) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\kappa^2 \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\kappa^2 > 0$ is the variance of the prior. Earlier in the course, we estimated **hyperparameters** like $\\kappa^2$ by optimizing the marginal likelihood. However, since we are dealing with a rather small dataset, we will be fully Bayesian and impose a **hyperprior** distribution on $\\kappa^2$. That is, we consider $\\kappa^2$ a random variable and impose a prior distribution on it. \n",
    "\n",
    "Specifically, we will use a **half-normal distribution** as prior for $\\kappa^2$:\n",
    "\\begin{align*}\n",
    "\\kappa^2 \\sim \\mathcal{N}_+(0, 1)\n",
    "\\end{align*}\n",
    "\n",
    "The half-normal distribution is just the \"positive-part\" of a standard normal distribution (https://en.wikipedia.org/wiki/Half-normal_distribution) and it serves to prevent to prior variance of $\\mathbf{w}$ from becoming too large such that we risk overfitting.\n",
    "\n",
    "To summarize, the full probabilistic model looks as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "t_n|\\mu_n &\\sim \\text{Poisson}(\\mu_n),\\\\\n",
    "\\mu_n &= \\exp(y_n)\\\\\n",
    "y_n &= \\mathbf{w}^T \\mathbf{x}_n\\\\\n",
    "\\mathbf{w}|\\kappa^2 &= \\mathcal{N}(0, \\kappa^2\\mathbf{I})\\\\\n",
    "\\kappa^2 &\\sim \\mathcal{N}_+(0, 1)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "One way to understand this model is to consider the \"generative story\" (via ancestral sampling):\n",
    "1. Nature samples a value for $\\kappa^2 \\sim \\mathcal{N}_+(0, 1)$.\n",
    "2. Using that specific value of $\\kappa^2$, nature picks a set of weights $\\mathbf{w}|\\kappa^2 = \\mathcal{N}(0, \\kappa^2\\mathbf{I})$.\n",
    "3. Using those specific weights, nature computes the mean values for all observations $\\mu_n = \\exp(\\mathbf{w}^T \\mathbf{x}_n)$.\n",
    "4. Finally, nature picks a value for each observation $t_n$ by sampling from $t_n|\\mu_n \\sim \\text{Poisson}(\\mu_n)$.\n",
    "\n",
    "The joint density of the model is given by\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t}, \\mathbf{w}, \\kappa^2) = \\prod_{n=1}^N p(t_n|\\mathbf{w})p(\\mathbf{w}|\\kappa^2)p(\\kappa^2),\n",
    "\\end{align*}\n",
    "\n",
    "where we have used $t_n|\\mathbf{w} \\sim \\text{Poisson}(\\exp(\\mathbf{w}^T \\mathbf{x}_n))$.\n",
    "\n",
    "\n",
    "The parameter $\\kappa^2 > 0$ is a positive parameter and to avoid problems with our sampler, we will reparametrize $\\kappa^2$ as $\\eta = \\log(\\kappa^2) \\in \\mathbb{R}$. Otherwise, we would risk that the Metropolis-Hastings algorithm would take a step, where $\\kappa^2$ would become negative. This **change-of-variables** introduces a correction term into the joint density:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t}, \\mathbf{w}, \\eta) = \\prod_{n=1}^N p(t_n|\\mathbf{w})p(\\mathbf{w}|\\kappa^2)p(\\exp(\\eta)) \\exp(\\eta)\n",
    "\\end{align*}\n",
    "\n",
    "accounting for the change $\\kappa^2 = \\exp(\\eta)$. \n",
    "\n",
    "Finally, we can implement a function for the log joint density of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 3\n",
    "\n",
    "def log_joint_poisson(theta):\n",
    "    \"\"\" Computes the log joint distribution for the Poisson regression model.\n",
    "    The argument theta is an np.array of shape (3), where the first two elements are the weights w and the last element is log(kappa2).\n",
    "    It returns the log joint distribution evaluated at theta (includes change-of-variable term).\n",
    "    \"\"\"\n",
    "    \n",
    "    # theta is three-dimensional, the first two entries represent the weights and the last represents eta = log(kappa^2)\n",
    "    w = theta[:2]\n",
    "    kappa2 = np.exp(theta[2])\n",
    "    \n",
    "    # log hyperprior and change-of-variable term\n",
    "    log_hyperprior = ?\n",
    "    \n",
    "    # log prior\n",
    "    log_prior = ?\n",
    "    \n",
    "    # log likelihood\n",
    "    log_likelihood = ?\n",
    "    \n",
    "    return log_likelihood + log_prior + log_hyperprior\n",
    "    \n",
    "num_iter = 20000\n",
    "warm_up = int(0.5*num_iter)\n",
    "\n",
    "thetas_poisson = metropolis(log_joint_poisson, num_params, 0.3, num_iter=num_iter)\n",
    "\n",
    "# get rid of warm-up samples\n",
    "#thetas_poisson = thetas_poisson[warm_up:, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Questions***\n",
    "\n",
    "3.1) Complete the implementation of the function *log_joint_poisson* above.\n",
    "\n",
    "*Hint*: The function *poisson.logpmf* will be useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the MCMC samples and plot the trace for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "w_samples = thetas_poisson[:, :2]\n",
    "eta_samples = thetas_poisson[:, 2]\n",
    "\n",
    "# transform\n",
    "kappa2_samples = np.exp(eta_samples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "axes[0].plot(w_samples[:, 0], 'r.')\n",
    "axes[0].set_title('$w_0$ (intercept)')\n",
    "\n",
    "axes[1].plot(w_samples[:, 1], 'r.')\n",
    "axes[1].set_title('$w_1$ (slope)')\n",
    "\n",
    "axes[2].plot(eta_samples, 'r.')\n",
    "axes[2].set_title('$\\\\eta = \\log(\\kappa^2)$')\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].set_xlabel('Iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "3.2) Inspect the trace for each parameter. Does it appear to have converged?\n",
    "\n",
    "\n",
    "3.3) Do you see the effect of the initial point?\n",
    "\n",
    "\n",
    "3.4) Uncomment the line that removes the warm-up samples above and re-do the plots\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare samples from the prior and the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "S = 10000\n",
    "\n",
    "# generate prior samples for kappa2 (w)\n",
    "kappa2_prior_samples = np.abs(np.random.normal(0, 1, size=S))\n",
    "\n",
    "# generate samples from w|kappa2\n",
    "w_prior_samples = np.random.normal(0, np.sqrt(kappa2_prior_samples)[:, None], size=(S, 2))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# samples of kappa^2\n",
    "axes[0].hist(kappa2_prior_samples, 20, density=True, color='r', alpha=0.5, label='Prior')\n",
    "axes[0].hist(kappa2_samples, 20, density=True, color='g', alpha=0.5, label='Posterior',  )\n",
    "\n",
    "axes[0].set_xlabel('$\\kappa^2$ (prior variance)')\n",
    "axes[0].set_title('Prior/posterior samples of $\\kappa^2$', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# samples of w\n",
    "axes[1].plot(w_prior_samples[:, 0], w_prior_samples[:, 1], 'r.', alpha=0.5, label='Prior')\n",
    "axes[1].plot(w_samples[:, 0], w_samples[:, 1], 'g.', label='Posterior')\n",
    "axes[1].legend(loc='upper left', markerscale=3)\n",
    "\n",
    "axes[1].set_xlabel('$w_0$ (intercept)')\n",
    "axes[1].set_ylabel('$w_1$ (slope)')\n",
    "axes[1].set_title('Prior/posterior samples of weights $\\mathbf{w}$', fontweight='bold');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "3.5) Inspect the two plots above and explain what you see. \n",
    "\n",
    "3.6) What is the  probability of the event $w_1 > 0$, i.e. the probability of the slope being positive, under both the prior and the posterior?\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have samples for the posterior over the weights, $p(\\mathbf{w} | \\mathbf{t})$, we can use it to compute and plot the predictive distribution for our Bayesian Poisson regression model using MC sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions by mapping the posterior samples of (w, kappa^2) through the model\n",
    "y_samples = ?\n",
    "mu_samples = ?\n",
    "t_samples = ?\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "plot_predictions(axes[0], age_pred, y_samples.T, num_samples=0, legend=True, title='Posterior of $y_*|\\mathbf{t}$')\n",
    "plot_predictions(axes[1], age_pred, mu_samples.T, num_samples=0, legend=True, title='Posterior of $\\mu_*|\\mathbf{t}$')\n",
    "plot_predictions(axes[2], age_pred, t_samples.T, num_samples=0, legend=True, title='Predictive distribution $t_*|\\mathbf{t}$')\n",
    "plot_data(axes[1])\n",
    "plot_data(axes[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "3.7) Compute and visualize the posterior distributions for $y_*, \\mu_*, t_*$ for $\\text{age} = 75$\n",
    "\n",
    "3.8) Report the mean and standard deviation of $p(t_*|\\mathbf{t})$ for $\\text{age} = 75$\n",
    "\n",
    "3.9) Compute the posterior probability of the event  $\\text{num. deaths} > 40$ for $\\text{age} = 75$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml2023",
   "language": "python",
   "name": "bml2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
