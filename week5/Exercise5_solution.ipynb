{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as snb\n",
    "\n",
    "from autograd.scipy.stats import norm\n",
    "\n",
    "from exercise5 import compute_err\n",
    "from exercise5 import eval_density_grid\n",
    "from exercise5 import load_MNIST_subset\n",
    "from exercise5 import generate_samples\n",
    "from exercise5 import plot_with_uncertainty\n",
    "from exercise5 import add_colorbar\n",
    "\n",
    "from exercise5 import GaussianProcessModel\n",
    "from exercise5 import NeuralNetworkMAP\n",
    "\n",
    "snb.set_style('darkgrid')\n",
    "snb.set(font_scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Bayesian Machine Learning: Non-Gaussian likelihoods and Gaussian processes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to\n",
    "- get familiar with Bayesian modelling with non-Gaussian likelihoods\n",
    "- study Gaussian processes for binary classification\n",
    "- understand how to compute predictive distributions for Monte Carlo methods\n",
    "\n",
    "The exercise is divided into two parts:\n",
    "- Part 1: Gaussian process classification for 1D toy data\n",
    "- Part 2: Gaussian process classification for 2D data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Gaussian process classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we will study Gaussian processes for binary classification using the following model\n",
    "\n",
    "$$\\begin{align*}\n",
    "t|y(\\mathbf{x}) &\\sim \\text{Ber}[\\sigma(y(\\mathbf{x}))]\\\\\n",
    "y(\\mathbf{x}) &\\sim \\mathcal{GP}(0, k(\\mathbf{x}, \\mathbf{x}')),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $k(\\mathbf{x}, \\mathbf{x}')$ is the covariance function and $\\sigma: \\mathbb{R} \\rightarrow (0, 1)$ is a suitable (inverse) link function. Here we will use the logistic sigmoid function. The purpose of the function $\\sigma$ is to \"squeeze\" the values of $y(\\mathbf{x})$ from the entire real line to the unit interval such that we can interpret $\\sigma(y(\\mathbf{x}))$ as a probability for all $\\mathbf{x}$.\n",
    "\n",
    "Our goal is to compute the predictive distribution $p(t^* = 1|\\mathbf{t}, \\mathbf{x}^*)$ for the class label $t^*$ of a new input point $\\mathbf{x}^*$. Just like for the Bayesian logistic regression model, exact analytical Bayesian inference is intractable for this model. Therefore, we will resort to the Laplace approximation again. \n",
    "\n",
    "\n",
    "Let $\\mathbf{t} \\in \\left\\lbrace 0, 1 \\right\\rbrace^N$ be the vector of binary observations and let $\\mathbf{y} = \\begin{bmatrix}y(\\mathbf{x}_1) & y(\\mathbf{x}_2) & \\dots & y(\\mathbf{x}_N) \\end{bmatrix} \\in \\mathbb{R}^N$ be the corresponding vector of latent function values for each sample in the training set. We will use the Laplace approximation to approximate the posterior distribution of the latent function values $\\mathbf{y}$ as follows\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{y}|\\mathbf{t}) = \\frac{p(\\mathbf{t}|\\mathbf{y})p(\\mathbf{y})}{p(\\mathbf{t})} \\approx q(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}| \\mathbf{m}, \\mathbf{S}) \\tag{1},\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mathbf{m}$ is the MAP solution (mode of the posterior) and $\\mathbf{S} = \\mathbf{A}^{-1}$ is approximated using the inverse Hessian matrix $\\mathbf{A}$ of the log joint density at the mode.\n",
    "\n",
    "Using the approximation $q$, we can compute the approximate posterior for $y^* = y(\\mathbf{x}^*)$ when evaluated at $\\mathbf{x}^*$ as follows\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*|\\mathbf{t}, \\mathbf{x}^*) &= \\int p(y^*|\\mathbf{y}, \\mathbf{x}^*) p(\\mathbf{y}|\\mathbf{t}) \\text{d} \\mathbf{y}\\approx  \\int p(y^*|\\mathbf{y}) q(\\mathbf{y}) \\text{d} \\mathbf{y} = \\mathcal{N}(y^*|\\mu_{y^*}, \\sigma^2_{y^*}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\mu_{y^*}$ and $\\sigma_{y^*}^2$ are the posterior mean and variance, respectively, for $y^*$.\n",
    "\n",
    "Finally, we can use this distribution to evaluate the predictive distribution. We will look at two different methods:  1) **Monte Carlo** sampling and 2) the **probit approximation**.\n",
    "\n",
    "**Monte Carlo** sampling works by generating a number of samples from the posterior of $y^*$, i.e. $y^*_{(i)} \\sim \\mathcal{N}(y^*|\\mu_{y^*}, \\sigma^2_{y^*})$ for $i = 1, \\dots, S$, and then computing sample mean\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(t^* = 1|\\mathbf{t}, \\mathbf{x}^*) \\approx \\int g(y^*) p(y^*|\\mathbf{t}) \\text{d} y^* \\approx \\frac{1}{S} \\sum_{i=1}^S g(y^*_{(i)}). \\tag{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The **probit approximation** approximates the sigmoid as follows: $\\sigma(y^*) \\approx \\Phi(y^* \\sqrt{\\frac{\\pi}{8}})$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution. The benefit is that the expectation value of the approximation can be computed analytically as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(t^* = 1|\\mathbf{t}, \\mathbf{x}^*) \\approx \\int g(y^*) p(y^*|\\mathbf{t}, \\mathbf{x}^*) \\text{d} y^* \\approx \\int \\phi\\left(y^* \\sqrt{\\frac{\\pi}{8}}\\right) p(y^*|\\mathbf{t}, \\mathbf{x}^*) \\text{d} y^* = \\Phi\\left(\\frac{\\mu_{y^*}}{\\sqrt{\\frac{8}{\\pi} + \\sigma^2_{y^*}}}\\right) \\tag{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we will work with a simple synthetic data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "sigmoid = lambda x: 1./(1+np.exp(-x))\n",
    "y = lambda x:  5*np.sin(0.75*x)\n",
    "\n",
    "N = 100\n",
    "X = np.random.normal(0, 2.5, size=N)[:, None]\n",
    "t = np.random.binomial(1, sigmoid(y(X)))\n",
    "\n",
    "# define points for prediction/plotting\n",
    "Xp = np.linspace(-15, 15, 300)[:, None]\n",
    "\n",
    "# plot data\n",
    "def plot_data(ax, X, t, title=\"Synthetic data\"):\n",
    "    ax.plot(X[t==0], t[t==0], 'ro', label='Class 0')\n",
    "    ax.plot(X[t==1], t[t==1], 'bo', label='Class 1')\n",
    "    ax.legend()\n",
    "    ax.set(xlabel='Input feature $x$', ylabel='Target $t$', title=title)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_data(ax, X, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik_bernoulli(y, t): \n",
    "    \"\"\" implement log p(t=1|y) using the sigmoid inverse link function \"\"\"\n",
    "    p = sigmoid(y)\n",
    "    return t.ravel()*np.log(p) + (1-t.ravel())*np.log(1-p)\n",
    "\n",
    "# fit the NN model with 1 input dimension, 2 hidden layers of 20 neurons each, and 1 output\n",
    "nn = NeuralNetworkMAP(X, t, [1, 20, 20, 1], alpha=1., log_lik_fun=log_lik_bernoulli)\n",
    "\n",
    "# predict using neural network\n",
    "y_nn = nn.predict(Xp)\n",
    "p_nn = sigmoid(y_nn)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_data(ax, X, t, title='Predictive probabilities for the neural network')\n",
    "ax.plot(Xp, p_nn, label='NN-MAP')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fit a Gaussian process model using the squared exponential kernel and using the Laplace approximation for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictive_prob_MC(mu_y, Sigma_y, sample_size=2000):\n",
    "    \"\"\"\n",
    "        The function computes p(t^* = 1|t, x^*) using Monte Carlo sampling  as in eq. (2).\n",
    "        The function also returns the samples generated in the process for plotting purposes\n",
    "\n",
    "        arguments:\n",
    "        mu_y             -- N x 1 vector\n",
    "        Sigma_y          -- N x N matrix\n",
    "        sample_size      -- positive integer\n",
    "\n",
    "        returns:\n",
    "        p                -- N   vector\n",
    "        y_samples        -- sample_size x N matrix\n",
    "        sigma_samples    -- sample_size x N matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # generate samples from y ~ N(mu, Sigma)\n",
    "    y_samples = generate_samples(mu_y, Sigma_y, sample_size).T \n",
    "\n",
    "    # apply inverse link function (elementwise)\n",
    "    sigma_samples = sigmoid(y_samples)\n",
    "\n",
    "    # return MC estimate, samples of y and sigma(y)\n",
    "    return np.mean(sigma_samples, axis=0), y_samples, sigma_samples\n",
    "\n",
    "# fit the GP model\n",
    "kappa = 2.\n",
    "scale = 1.\n",
    "theta = [kappa, scale]\n",
    "gp = GaussianProcessModel(X, t, theta, log_lik_bernoulli)\n",
    "\n",
    "# compute distribution p(y^*|t) = N(y^*|mu_y, Sigma_y)\n",
    "mu_y, Sigma_y = gp.compute_posterior_y(Xp, pointwise=False)\n",
    "var_y = np.diag(Sigma_y)\n",
    "\n",
    "# compute prediction distribution p(t^*|t, x^*)\n",
    "p, y_samples, p_samples = compute_predictive_prob_MC(mu_y, Sigma_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "\n",
    "1.1) Skim through the code of the classes *NeuralNetworkMAP* and *GaussianProcessModel* in the module *exercise5.py*\n",
    "\n",
    "\n",
    "\n",
    "1.2) Complete the implementation of the function *compute_predictive_prob_MC* above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots and compares the neural network and Gaussian process model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution for question 1.7)\n",
    "phi = lambda x: norm.cdf(x)\n",
    "\n",
    "def probit_approximation(mu_y, Sigma_y):\n",
    "    return phi(mu_y.ravel()/np.sqrt(8/np.pi + np.diag(Sigma_y)))\n",
    "    \n",
    "probit_p = probit_approximation(mu_y, Sigma_y)\n",
    "\n",
    "# number of samples function to plot\n",
    "num_samples_plot = 10\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(25, 6))\n",
    "plot_data(ax[0], X, t)\n",
    "plot_data(ax[1], X, t)\n",
    "\n",
    "# plot posterior  p(y^*|t, x^*)\n",
    "plot_with_uncertainty(ax[0], Xp, mu_y, Sigma_y, color='g', title='Posterior distribution $p(y^*(x)|\\mathbf{t}, x^*)$')\n",
    "ax[0].plot(Xp, y_samples[:num_samples_plot, :].T, color='g', alpha=0.2)\n",
    "ax[0].plot(Xp, y_nn, 'y', linewidth=3, label='NN')\n",
    "ax[0].plot(X, gp.m, 'gx')\n",
    "ax[0].legend(ncol=4)\n",
    "ax[0].set_ylabel('y(x)')\n",
    "\n",
    "# plot predictive distribution p(t^*|t, x^*)\n",
    "ax[1].plot(Xp, p_samples[:num_samples_plot, :].T, color='g', alpha=0.2)\n",
    "ax[1].plot(Xp, p, 'g-', linewidth=3, label='GP-LAP (MC)')\n",
    "ax[1].plot(Xp, p_nn, 'y', linewidth=3, label='NN-MAP')\n",
    "ax[1].plot(Xp, probit_p, 'r--', label='GP-LAP (probit)')\n",
    "ax[1].set(ylim=(-0.05, 1.3), title='Posterior predictive $p(t^*|\\mathbf{t}, x^*)$')\n",
    "ax[1].legend(ncol=4, loc='upper center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Questions***\n",
    "\n",
    "\n",
    "1.3) What is the relationship between the thin green lines on the left hand plot vs the right hand plot?\n",
    "\n",
    "1.4) What are the dimensions of the mean $\\mathbf{m}$ and the covariance $\\mathbf{S}$ of the Laplace approximation?\n",
    "\n",
    "1.5) What does the values of the mean $\\mathbf{m}$ in the Laplace approximation $q(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{m}, \\mathbf{S})$ represent and how does it relate to the figures?\n",
    "\n",
    "*Hint*: If you are in doubt, plot the posterior mean vector *gp.m* vs. training data *X* on top of the left plot\n",
    "\n",
    "1.6) Compare the predictive distribution for the neural network and the Gaussian process. How do they differ (qualitatively) within the support of the data?\n",
    " Outside the support of the data?\n",
    "\n",
    " *Hint*: Increase the prediction range for $X_p$, e.g. from $\\left[-6.5, 6.5\\right]$ to $\\left[-15, 15\\right]$\n",
    " \n",
    "1.7) Implement and plot the probit approximation in eq. (3) for the Gaussian process. \n",
    "\n",
    "*Hint*: the function *norm.cdf* implements the cumulative distribution function for the standardized Gaussian distribution\n",
    "\n",
    "1.8) What are the pros and cons for the probit approximation vs the Monte Carlo sampling?\n",
    "\n",
    "\n",
    "1.9) The neural network model uses an isotropic Gaussian prior for all weights, i.e. $w_i \\sim \\mathcal{N}\\left(0, \\alpha^{-1}\\right)$. What happens if you increase $\\alpha$ from $1$ to e.g. $10$? Can you explain the result?\n",
    "\n",
    "*Hint*: Recall the relationship between Gaussian priors and regularization when using MAP estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Gaussian process classification for 2D data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the MNIST data from week 3 to further study the properties of Gaussian process models.\n",
    "\n",
    "The function in the cell below loads the MNIST data and projects the data to a 2-dimensional space using PCA as we did in week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [4, 7]\n",
    "Xtrain, Xtest, ttrain, ttest = load_MNIST_subset('./mnist_subset.npz', digits=digits, subset=500)\n",
    "\n",
    "# make sure dimensions are [N x 1]\n",
    "ttrain = ttrain[:, None]\n",
    "ttest = ttest[:, None]\n",
    "\n",
    "X = np.row_stack((Xtrain, Xtest))\n",
    "t = np.row_stack((ttrain, ttest))\n",
    "\n",
    "# plot\n",
    "def plot_data(ax, title=\"\", xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    ax.plot(Xtrain[:, 0], Xtrain[:, 1], 'ko', markersize=7, label='Training data')\n",
    "    ax.plot(X[t.ravel()==0, 0], X[t.ravel()==0, 1], 'b.', label='Digit %d' % digits[0])\n",
    "    ax.plot(X[t.ravel()==1, 0], X[t.ravel()==1, 1], 'r.', label='Digit %d' % digits[1])\n",
    "    ax.set_xlim((-5,5 ))\n",
    "    ax.set_ylim((-5,5 ))\n",
    "    ax.set(title=title, xlim=xlim, ylim=ylim, xlabel='PC1', ylabel='PC2')\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,9))\n",
    "plot_data(ax, title='Subset of MNIST data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process\n",
    "kappa = 1.\n",
    "scale = 1.\n",
    "theta = [kappa, scale]\n",
    "\n",
    "gp = GaussianProcessModel(Xtrain, ttrain, theta, log_lik_bernoulli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive(Xp):\n",
    "    mu_y, var_y = gp.compute_posterior_y(Xp, pointwise=True)\n",
    "    p, _, _ = compute_predictive_prob_MC(mu_y, np.diag(var_y))\n",
    "    return p\n",
    "\n",
    "# predict\n",
    "p_train = predictive(Xtrain)\n",
    "p_test = predictive(Xtest)\n",
    "\n",
    "# make predictions\n",
    "ttrain_hat = 1.0*(p_train > 0.5)\n",
    "ttest_hat = 1.0*(p_test > 0.5)\n",
    "\n",
    "# print results: mean and standard error of the mean\n",
    "print('Training error:\\t%3.2f (%3.2f)' % compute_err(ttrain_hat.ravel(), ttrain.ravel()))\n",
    "print('Test error:\\t%3.2f (%3.2f)' % compute_err(ttest_hat.ravel(), ttest.ravel()))\n",
    "\n",
    "# prepare for plotting\n",
    "x_grid, posterior_y_gpc_eval_mu = eval_density_grid(lambda x: gp.compute_posterior_y(x)[0], P=50, )\n",
    "x_grid, posterior_y_gpc_eval_var = eval_density_grid(lambda x: gp.compute_posterior_y(x)[1], P=50)\n",
    "x_grid, posterior1_gpc_eval = eval_density_grid(predictive, P=50, a=-5, b=5)\n",
    "\n",
    "# prepare plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(25, 6))\n",
    "\n",
    "# plot posterior mean\n",
    "im = ax[0].pcolormesh(x_grid, x_grid, posterior_y_gpc_eval_mu, cmap=plt.cm.RdBu_r, norm=colors.CenteredNorm(), shading='auto')\n",
    "plot_data(ax[0], title=\"Posterior mean of $y^*$\")\n",
    "add_colorbar(im, fig, ax[0])\n",
    "\n",
    "# plot posterior var\n",
    "im = ax[1].pcolormesh(x_grid, x_grid, posterior_y_gpc_eval_var, shading='auto')\n",
    "plot_data(ax[1], title=\"Posterior variance of $y^*$\")\n",
    "add_colorbar(im, fig, ax[1])\n",
    "\n",
    "# plot posterior predictive\n",
    "im = ax[2].pcolormesh(x_grid, x_grid, posterior1_gpc_eval, cmap=plt.cm.RdBu_r, shading='auto', clim=(0, 1))\n",
    "plot_data(ax[2], title=\"Predictive prob for $t^* = 1$\")\n",
    "add_colorbar(im, fig, ax[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "2.1) Explain what you see in the 3 panels - explain the difference between the two distributions $p(y(\\mathbf{x})|\\mathbf{t})$ and $p(t(\\mathbf{x})|\\mathbf{t})$\n",
    "\n",
    "2.2) What is the dimensionality of $\\mathbf{m}$ and $\\mathbf{S}$?\n",
    "\n",
    "2.3) What does the values of mean $\\mathbf{m}$ in the Laplace approximation $q(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{m}, \\mathbf{S})$ represent and how does it relate to the figures?\n",
    "\n",
    "2.4) Comment on the uncertainties within the support of the data and outside the data\n",
    "\n",
    "2.5) Make the equivalent of the right most plot for the NN neural model below. Comment on the differences.\n",
    "\n",
    "*Hint*: Remember to change the number of inputs in the neural network from 1 to 2\n",
    "\n",
    "2.6) What is the training and test error for the neural network?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to 2.4\n",
    "nn = NeuralNetworkMAP(Xtrain, ttrain, [2, 20, 20, 1], alpha=1., log_lik_fun=log_lik_bernoulli)\n",
    "\n",
    "x_grid, posterior1_nn_eval = eval_density_grid(lambda x: sigmoid(nn.predict(x)), P=50, a=-5, b=5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "im = ax[0].pcolormesh(x_grid, x_grid, posterior1_nn_eval, cmap=plt.cm.RdBu_r, shading='auto')\n",
    "plot_data(ax[0], title='Predictive prob for t = 1 for NN')\n",
    "add_colorbar(im, fig, ax[0])\n",
    "\n",
    "plot_data(ax[1], title='Posterior predictive for GP')\n",
    "im = ax[1].pcolormesh(x_grid, x_grid, posterior1_gpc_eval, cmap=plt.cm.RdBu_r, shading='auto')\n",
    "add_colorbar(im, fig, ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution to 2.6)\n",
    "\n",
    "# make predictions for y\n",
    "ytrain = nn.predict(Xtrain)\n",
    "ytest = nn.predict(Xtest)\n",
    "\n",
    "# make predictions for t\n",
    "ttrain_hat = 1.0*(ytrain > 0)\n",
    "ttest_hat = 1.0*(ytest > 0)\n",
    "\n",
    "# print results: mean and standard error of the mean\n",
    "print('Training error:\\t%3.2f (%3.2f)' % compute_err(ttrain_hat.ravel(), ttrain.ravel()))\n",
    "print('Test error:\\t%3.2f (%3.2f)' % compute_err(ttest_hat.ravel(), ttest.ravel()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml2023",
   "language": "python",
   "name": "bml2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
