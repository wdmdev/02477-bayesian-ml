{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02477 Bayesian Machine Learning - Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "The topic of this exercise is **multi-class classification** and **decision theory**. Probability theory and Bayes' rule tell us how to summarize our knowledge about a parameter or a prediction using probability distributions.  However, often we have to reduce these distributions a single decision, e.g. does the patient have cancer or not, and decision theory tells us how to do that in a principled manner. We will see how the **posterior predictive probabilities** play a key role in making **optimal decisions** and see how the choice of **utility function** affects the resulting decisions. We will also look at how to quantify and represent the predictive uncertainty for multiclass classification and how to investigate the accuracy of the posterior predictive probabilities.\n",
    "\n",
    "We will study these concepts using **Bayesian linear models** for multi-class classification problems.\n",
    "\n",
    "The exercise is divided into four parts:\n",
    "\n",
    "- Part 1: Warm up: Linear models for multi-class classification\n",
    "\n",
    "\n",
    "- Part 2: Bayesian decision theory for classification\n",
    "\n",
    "\n",
    "- Part 3: Image classification\n",
    "\n",
    "\n",
    "- Part 4: Calibration\n",
    "\n",
    "\n",
    "Note that the in this exercise, we will need the python package called **PIL** for manipulating images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as snb\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from autograd import value_and_grad\n",
    "\n",
    "# for plotting\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# for manipulating images\n",
    "from PIL import Image\n",
    "\n",
    "from exercise6 import PCA_dim_reduction\n",
    "from exercise6 import visualize_utility\n",
    "from exercise6 import to_onehot\n",
    "from exercise6 import BayesianLinearSoftmax\n",
    "\n",
    "# style stuff\n",
    "snb.set(font_scale=1.5)\n",
    "snb.set_style('darkgrid')\n",
    "colors = ['r', 'g', 'b', 'y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1: Bayesian linear models for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exericse 3 we used a logistic regression model for binary classification and in this exercise, we will work with the natural extension for multi-class classification.\n",
    "\n",
    "Consider a multi-class dataset with $K$ classes and let $\\mathcal{D} = \\left\\lbrace \\mathbf{x}_i, t_i \\right\\rbrace_{i=1}^N$ denote a dataset, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ and $t_n \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace$ are the input feature and target label, respectively, for the $i$'th example.\n",
    "\n",
    "As likelihood, we will use a categorical distribution\n",
    "\n",
    "\\begin{align*}\n",
    "t_n|\\mathbf{y}_n &\\sim \\text{Categorical}\\left[\\text{softmax}(\\mathbf{y}_n)\\right], \\tag{1} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "which specifies the probability of observing each class given a set of latent function values ${\\mathbf{y}_n} \\in \\mathbb{R}^K$ for $\\mathbf{x}_n$. The latent function values are modeled using linear functions:\n",
    "\n",
    "\\begin{align*}\n",
    "[\\mathbf{y}_n]_i &= \\mathbf{w}_i^T \\phi(\\mathbf{x}_n) \\tag{2}\\\\\n",
    "\\mathbf{w}_i &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}),\n",
    "\\end{align*}\n",
    "\n",
    "for $i = 1, \\dots, K$. The vector $\\mathbf{y}_n \\in \\mathbb{R}^K$ in eq. (2) contains the values of the latent linear functions for all $K$ classes, i.e. $\\mathbf{y}_n = \\left[y^1(\\mathbf{x}_n), y^2(\\mathbf{x}_n), \\dots, y^K(\\mathbf{x}_n)\\right]$, where the $i$'th function is represented using the vector $\\mathbf{w}_i$. We use isotropic Gaussian priors for all the parameters of the models.\n",
    "\n",
    "We will again resort to the Laplace approximation for inference because the posterior distribution is analytically intractable and use **Monte Carlo** sampling to estimate the posterior predictive probabilities.\n",
    "\n",
    "**Note:** In the code we will represent the class labels as integers from $0, 1, \\dots, K-1$ rather than from $1, \\dots, K$ because Python counts from 0.\n",
    "\n",
    "We will start with a small toy example with $K = 4$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# number of data points per class\n",
    "M = 50\n",
    "\n",
    "# generate simple synthetic toy dataset\n",
    "xi = [np.random.normal(-3, 1, size=(M, 1)),\n",
    "      np.random.normal(-1, 1, size=(M, 1)),\n",
    "      np.random.normal(1, 1, size=(M, 1)),\n",
    "      np.random.normal(3, 1, size=(M, 1))]\n",
    "x = np.row_stack((xi))\n",
    "t = np.hstack((np.zeros(M), np.ones(M), 2*np.ones(M), 3*np.ones(M)))\n",
    "num_classes = 4\n",
    "\n",
    "# specify input points for predictions\n",
    "xpred = np.linspace(-5, 5, 300)\n",
    "\n",
    "# include intercept\n",
    "design_matrix = lambda x: np.column_stack((np.ones(len(x)), x))\n",
    "\n",
    "# fit model and compute predictions\n",
    "bls = BayesianLinearSoftmax(design_matrix(x), t)\n",
    "mu_y, var_y = bls.compute_posterior_y(design_matrix(xpred))\n",
    "phat = bls.compute_predictive_prob(design_matrix(xpred))\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1,3, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    # plot histogram of data\n",
    "    axes[0].hist(xi[i], density=True, label='Class %d' % i, alpha=0.75, color=colors[i]);\n",
    "    # plot posterior mean of latent function y for each class\n",
    "    axes[1].plot(xpred, mu_y[:, i], label='i = %d' % i, color=colors[i])    \n",
    "    # plot posterior class probabilitites\n",
    "    axes[2].plot(xpred, phat[:, i], label='i = %d' % i, color=colors[i])\n",
    "    \n",
    "axes[0].legend()\n",
    "for i in range(3):\n",
    "    axes[i].set_xlabel('Input x')\n",
    "    \n",
    "axes[0].set_title('Data')\n",
    "axes[1].set_title('Posterior mean for $y_i$')\n",
    "axes[2].set_title('Posterior class probabilities $p(t=i|x)$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions**\n",
    "\n",
    "1.1) Explain the role of the softmax-function in eq. (1) above and use the figures above to explain why the name \"softmax\" makes sense\n",
    "\n",
    "1.2) Implement and plot the **confidence** and **entropy** for the predictive distributions above for each value of $x$ in the vector *xpred* (similar to the rightmost plot above). Comment on the relation between the confidence and entropy plots and the posterior predictive probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### solution for 1.2\n",
    "def entropy(p):\n",
    "    \"\"\" assumes p is [N, K] where N is the number of prediction points and K is the number of classes \"\"\"\n",
    "    return -np.sum(p*np.log(p), 1)\n",
    "\n",
    "def confidence(p):\n",
    "    \"\"\" assumes p is [N, K] where N is the number of prediction points and K is the number of classes \"\"\"\n",
    "    return np.max(p, 1)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "ax[0].plot(xpred, confidence(phat), linewidth=3)\n",
    "ax[0].set(xlabel='Input x', ylabel='Confidence')\n",
    "ax[1].plot(xpred, entropy(phat), linewidth=3)\n",
    "ax[1].set(xlabel='Input x', ylabel='Predictive entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Bayesian decision theory for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a decision for multi-class classification entails assigning a class label $\\hat{t} \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace$ to a new test point $\\mathbf{x}_*$. In Bayesian decision theory, the utility function $\\mathcal{U}(t_*, \\hat{t})$ specifies the **utility** (i.e. gain) for predicting $\\hat{t}$ when the true target is $t_*$.\n",
    "\n",
    "In practice, we don't know the true target $t_*$, but the predictive posterior distribution $p(t_*|\\mathbf{t}, \\mathbf{x}_*)$ contains all the relevant knowledge about $t_*$ given our data observed $\\mathbf{t}$. Therefore, we compute the **expected utility** wrt. the posterior predictive distribution for each possible value of $\\hat{t}$ and then assign the class label that **maximizes the expected utility**. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{t} = \\arg\\max\\limits_{i \\in \\left\\lbrace 1, 2, \\dots, K\\right\\rbrace} \\mathbb{E}_{p(t_*|\\mathbf{t}, \\mathbf{x}_*)}\\left[\\mathcal{U}(t_*, \\hat{t}=i)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "In the lecture we saw that in order to make optimal decisions under the **0/1-utility function** (remember utility is just negative loss and vice versa), we simply have to pick the class label with largest posterior probability. Let's investigate this empirically and study how the decision regions change when we change the utility-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define utility matrix\n",
    "U = np.identity(num_classes)\n",
    "\n",
    "#######################################\n",
    "# Solution for question 2.4\n",
    "#######################################\n",
    "#U = np.identity(num_classes)\n",
    "#U[0, 1] = -1\n",
    "#U[0, 1] = -2\n",
    "\n",
    "#######################################\n",
    "# Solution for question 2.5\n",
    "#######################################\n",
    "#U = np.identity(num_classes)\n",
    "#U[2, 3] = U[3,2] = 1\n",
    "\n",
    "#######################################\n",
    "# Solution for question 2.6\n",
    "#######################################\n",
    "#U = np.identity(num_classes)\n",
    "#U[1, 1] = 0.5\n",
    "\n",
    "\n",
    "# compute the expected utility for each class\n",
    "expected_utility = phat@U\n",
    "\n",
    "# make decisions\n",
    "decisions = to_onehot(np.argmax(expected_utility, axis=1), num_classes)\n",
    "    \n",
    "# plot everything\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# utility matrix\n",
    "visualize_utility(axes[0,0], U)\n",
    "\n",
    "# posterior predictive probs\n",
    "for i in range(num_classes):\n",
    "    axes[0,1].plot(xpred, phat[:, i], color=colors[i])\n",
    "axes[0,1].set_title('Posterior predictive probabilities', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Input x')\n",
    "\n",
    "# expected utility\n",
    "for i in range(num_classes):\n",
    "    axes[1,0].plot(xpred, expected_utility[:, i], color=colors[i]);\n",
    "axes[1,0].set_title('Expected utility for each class', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Input x')\n",
    "\n",
    "# decisions\n",
    "for i in range(num_classes):\n",
    "    axes[1,1].plot(xpred, decisions[:, i], color=colors[i], linewidth=2);\n",
    "    axes[1,1].plot(xpred, phat[:, i], color=colors[i], alpha=0.5, linestyle='--')\n",
    "axes[1,1].set_title('Decision regions', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Input x')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "2.1) Use the figure above to explain how Bayesian decision theory works for multi-class classification.\n",
    "\n",
    "2.2) Explain how the expected utilities are calculated.\n",
    "\n",
    "2.3) What happens to the expected utilities and to the decisions if you scale the utility matrix by a positive constant?\n",
    "\n",
    "2.4) What happens to the decision boundary if you introduce a negative utility of $-1$ for predicting 1 (green), when the true target is 0 (red)? What about $-2$?\n",
    "\n",
    "2.5) What happens if you change the 0/1 utility function to have $U_{23} = U_{32} = 1$?\n",
    "\n",
    "2.6) What happens if you change the 0/1 utility function to have $U_{11} = 0$? or to $U_{11} = 0.5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to apply the material from part 1 and part 2 to a real dataset. Specifically, we will work with a subset of the Linnaeus 5 dataset (http://chaladze.com/l5/). The original Linnaeus 5 dataset contains images of size 256x256 from 5 classes, but we will work with a subset of this dataset containing a total of 3200 images in 4 classes (dogs, birds, flowers, berries). The images have been resized to 128x128 with the sole purpose of reducing the size of the data file. \n",
    "\n",
    "We will use **transfer learning** and use a pretrained ResNet18-network as a **feature extractor** for the images. ResNet18 is a convolutional neural network with 18 layers, which has been trained on a huge image dataset called ImageNet. The ImageNet containes images from 1000 different classes, which means that the very last layer of the ResNet architecture is a softmax-layer with 1000 outputs. However, if we get rid of the very last layer, we can use the rest of the network as a general feature extractor for images. That is, we propagate each image through the network we can use the very last hidden layer as a 512-dimensional feature vector for the image.\n",
    "\n",
    "The details of how this works beyond what's written above is **not** part of the curriculum of the course and therefore, we have pre-computed feature vectors for all the images for you. \n",
    "\n",
    "However, if you are interested in the details, you can look at the following resources:\n",
    "\n",
    "- Code used for feature extraction [here](https://github.com/christiansafka/img2vec)\n",
    "- Paper describing the ResNet architecture [here](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- A Pytorch tutorial for transfer learning for vision problems [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n",
    "\n",
    "\n",
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load('./ex6_data.npz')\n",
    "labels = list(data['labels'])\n",
    "targets = data['targets']\n",
    "num_classes = data['num_classes'][()]\n",
    "\n",
    "Xtrain, Xtest = data['Xtrain'], data['Xtest']\n",
    "ttrain, ttest = data['ttrain'], data['ttest']\n",
    "train_idx = data['train_idx']\n",
    "test_idx = data['test_idx']\n",
    "\n",
    "N, D = Xtrain.shape\n",
    "Ntest = len(Xtest)\n",
    "print(f'Number of images for training: {N}')\n",
    "print(f'Number of images for test: {Ntest}')\n",
    "print(f'Number of features: {D}')\n",
    "print(f'Number of clases: {num_classes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and plot a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_example(ax, i):\n",
    "    \"\"\" show training example i \"\"\"\n",
    "    \n",
    "    j = train_idx[i]\n",
    "    img = Image.open('./images/%d.jpg' % j)\n",
    "    target = targets[j]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(labels[int(target)])\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, ax = plt.subplots(2, 8, figsize=(20, 6))\n",
    "for i in range(16):\n",
    "    show_example(ax.flat[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is represented using a 512-dimensional feature vector, but we will (again) reduce the dimensionality to $D = 2$ using principal component analysis (PCA) for the purpose of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ztrain, Ztest = PCA_dim_reduction(Xtrain, Xtest, num_components=2)\n",
    "\n",
    "def plot_pca_data(ax, legend=True, alpha=1):\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ax.plot(Ztest[ttest==i, 0], Ztest[ttest==i, 1], '.', color='k', markersize=14, alpha=0.6)\n",
    "        ax.plot(Ztest[ttest==i, 0], Ztest[ttest==i, 1], '.', color=colors[i], label=labels[i], markersize=6, alpha=alpha)\n",
    "    if legend:\n",
    "        ax.legend(markerscale=2)\n",
    "    ax.set(xlabel='PC1', ylabel='PC2')\n",
    "    ax.set_title('Test data')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pca_data(ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  fit the model and visualize the posterior class probabilitites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_regions(x_grid, posterior_class_probs, name, show_data=True):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))\n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        probs_reshaped =  posterior_class_probs[:, i].reshape((len(x_grid), len(x_grid)))\n",
    "        im = axes[i].pcolormesh(x_grid, x_grid,probs_reshaped, cmap=plt.cm.RdBu_r, clim=(0, 1), shading='auto')\n",
    "\n",
    "        if show_data:\n",
    "            plot_pca_data(axes[i], legend=False)\n",
    "        axes[i].set_title('Class %d: %s' % (i, labels[i]))\n",
    "\n",
    "        if i > 0:\n",
    "            axes[i].set_yticklabels([])\n",
    "            axes[i].set_ylabel('')\n",
    "\n",
    "    fig.subplots_adjust(right=0.9, wspace=0.01)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.025, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax);\n",
    "    fig.suptitle(name, fontweight='bold', y=1.025)\n",
    "        \n",
    "# we want to have an intercept in the model\n",
    "Phi_train, Phi_test = design_matrix(Ztrain), design_matrix(Ztest)\n",
    " \n",
    "# prepare grid for making predictions\n",
    "x_grid = np.linspace(-3.5, 3.5, 100)\n",
    "XX1, XX2 = np.meshgrid(x_grid, x_grid)\n",
    "Xp = np.column_stack((XX1.ravel(), XX2.ravel()))\n",
    "Phi_pred = design_matrix(Xp)\n",
    "\n",
    "# Fit linear classifier\n",
    "bls = BayesianLinearSoftmax(Phi_train, ttrain)\n",
    "p_pred_bls = bls.compute_predictive_prob(Phi_pred)\n",
    "\n",
    "# visualize the posterior class proabilities for each model\n",
    "visualize_regions(x_grid, p_pred_bls, 'Posterior class probabilities for Bayesian Linear Softmax model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "3.1) Compute predictions (wrt. the 0/1-utility function) and compute the accuracy for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to 3.1\n",
    "def compute_accuracy(t_true, t_pred):\n",
    "    return np.mean(t_true.ravel() == t_pred.ravel())\n",
    "\n",
    "# evalaute class probabilities for training and test set using Bayesian linear softmax\n",
    "p_train_bls = bls.compute_predictive_prob(Phi_train)\n",
    "p_test_bls = bls.compute_predictive_prob(Phi_test)\n",
    "\n",
    "# classify using 0/1 utility function\n",
    "ttrain_hat_bls = np.argmax(p_train_bls, axis=1)\n",
    "ttest_hat_bls = np.argmax(p_test_bls, axis=1)\n",
    "\n",
    "# compute and print accuracy\n",
    "print('\\tAccuracy training:\\t%3.2f' % compute_accuracy(ttrain, ttrain_hat_bls))\n",
    "print('\\tAccuracy test:\\t\\t%3.2f' % compute_accuracy(ttest, ttest_hat_bls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) Compute and plot the **entropy** and **confidence** for the predictions in the plot above. In which areas of the input space is this model most uncertain about the class label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Soluton to 3.2\n",
    "def confidence(p):\n",
    "    return np.max(p, 1)\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p*np.log(p), 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "conf_ax = ax[0].pcolormesh(x_grid, x_grid, confidence(p_pred_bls).reshape((len(x_grid), len(x_grid))), shading='auto', cmap=plt.cm.RdBu_r)\n",
    "ax[0].set_title('Confidence')\n",
    "ax[1].pcolormesh(x_grid, x_grid, entropy(p_pred_bls).reshape((len(x_grid), len(x_grid))), shading='auto', cmap=plt.cm.RdBu_r)\n",
    "ax[1].set_title('Entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) Compute the compute the average confidence for the training set and test set and compare to the results from 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Avg. confidence training: %3.2f' % np.mean(confidence(p_train_bls)))\n",
    "print('Avg. confidence test: %3.2f' % np.mean(confidence(p_test_bls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making decisions with a reject option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will investigate how to make decisions with a **reject** option, meaning we avoid to making any decisions if the confidence is below a specified threshold $p_{\\text{reject}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify reject threshold\n",
    "p_reject = 0.6\n",
    "\n",
    "# compute decisions under 0/1-utility function and reshape to grid\n",
    "decisions_bls = np.argmax(p_pred_bls, axis=1).reshape((len(x_grid), len(x_grid)))\n",
    "\n",
    "# identify reject regions\n",
    "p_pred_confidence = confidence(p_pred_bls)\n",
    "reject_region_bls = 1.0*np.logical_not((p_pred_confidence < p_reject).reshape((len(x_grid), len(x_grid))))\n",
    "decisions_bls_with_reject = (decisions_bls+1)*reject_region_bls\n",
    "\n",
    "# visualize\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 9))\n",
    "plot_pca_data(axes[0])\n",
    "axes[0].pcolormesh(x_grid, x_grid, decisions_bls, cmap=ListedColormap(colors), alpha=1, shading='auto')\n",
    "axes[0].set_title('Decision regions for Bayesian Linear softmax')\n",
    "plot_pca_data(axes[1]);\n",
    "axes[1].pcolormesh(x_grid, x_grid, decisions_bls_with_reject, cmap=ListedColormap(['k'] + colors), shading='auto')\n",
    "axes[1].set_title('Decision regions w. reject for Bayesian Linear Softmax');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "3.4) Explain what you see in the figure above - relate the **reject region** in black to the confidence plot in the figure from 3.2\n",
    "\n",
    "3.5) What happens to the reject region if you increase or decrease the reject threshold?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6) How many percent of the samples in the test set are rejected with $p_{\\text{reject}} = 0.6$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution to 3.6\n",
    "test_set_reject_idx = confidence(p_test_bls) < p_reject\n",
    "test_set_keep_idx = confidence(p_test_bls) >= p_reject\n",
    "print('Fraction of sample rejected: %3.2f' %np.mean(test_set_reject_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.7) What is the test accuracy rate for the samples in the test set, which are not rejected?\n",
    "\n",
    "3.8) If we were to make a decision for the test samples in the reject region, what would the accuracy rate be? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution to 3.7 and 3.8\n",
    "ttest_reject = ttest[test_set_reject_idx]\n",
    "ptest_reject = p_test_bls[test_set_reject_idx]\n",
    "ttest_reject_hat = np.argmax(ptest_reject, axis=1)\n",
    "\n",
    "ttest_keep = ttest[test_set_keep_idx]\n",
    "ptest_keep = p_test_bls[test_set_keep_idx]\n",
    "ttest_keep_hat = np.argmax(ptest_keep, axis=1)\n",
    "\n",
    "print('Accuracy for rejected samples:\\t\\t%3.2f' % compute_accuracy(ttest_reject, ttest_reject_hat))\n",
    "print('Accuracy for kept samples:\\t\\t%3.2f' % compute_accuracy(ttest_keep, ttest_keep_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.9) Instead, if the reject option, implement the following utility function and plot the corresponding decision boundaries:\n",
    "\n",
    "- Utility of 1 for correctly classifying dogs, birds, and flowers\n",
    "- Utility of 2 for correctly classifying berries\n",
    "- Utility of 0 for all misclassifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution for 3.9\n",
    "\n",
    "# specify utility  functions\n",
    "U = np.identity(4)\n",
    "U[2,2] = 2\n",
    "\n",
    "# compute the expected utility for each class for each utility function\n",
    "expected_utility = p_pred_bls@U\n",
    "\n",
    "# make decisions and reshape\n",
    "decisions = np.argmax(expected_utility, axis=1).reshape((len(x_grid), len(x_grid)))\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "visualize_utility(axes[0], U, labels=labels)\n",
    "axes[1].pcolormesh(x_grid, x_grid, decisions, cmap=ListedColormap(colors), alpha=1, shading='auto')\n",
    "axes[1].set_title('Decision regions for $U_{%d}$' % (i+1))\n",
    "plot_pca_data(axes[1]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the posterior predictive probabilities play a key role in decision making. In the last part of this exercise, we will study calibration of these probabilities. We will study the calibration of the individual posterior predictive class probabilities $p(t_*=i|\\mathbf{t},\\mathbf{x}^*)$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_calibration_curve(targets, probs, num_bins=10):\n",
    "\n",
    "    bins = np.linspace(0, 1, num_bins+1)\n",
    "\n",
    "    p_true_mean, p_true_se, p_pred = [], [], []\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        bin_start, bin_end = bins[i], bins[i+1]\n",
    "        bin_center = 0.5*(bin_start + bin_end)\n",
    "        \n",
    "        bin_idx = np.logical_and(bin_start <= probs, probs < bin_end)\n",
    "        num_points_in_bin = np.sum(bin_idx)\n",
    "        \n",
    "        if len(targets[bin_idx]) == 0:\n",
    "            continue\n",
    "\n",
    "        p_pred.append(bin_center)\n",
    "        p_est = np.mean(targets[bin_idx])\n",
    "        p_true_mean.append(p_est)\n",
    "        p_true_se.append(np.sqrt(p_est*(1-p_est)/num_points_in_bin))\n",
    "        \n",
    "    return np.array(p_true_mean), np.array(p_true_se), np.array(p_pred)\n",
    "\n",
    "p_test_bls = bls.compute_predictive_prob(Phi_test)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "for class_idx in range(num_classes):\n",
    "    ax = axes.flatten()[class_idx]\n",
    "    p_true_mean, p_true_se, p_pred = compute_calibration_curve(ttest==class_idx, p_test_bls[:, class_idx])\n",
    "    ax.errorbar(p_pred, p_true_mean, p_true_se, label=labels[class_idx], color=colors[class_idx], marker='o', markersize=8)\n",
    "    ax.plot([0, 1], [0, 1], 'k-', alpha=0.4)\n",
    "    ax.legend()\n",
    "    \n",
    "\n",
    "axes[1,0].set_xlabel('Predicted probability')\n",
    "axes[1,1].set_xlabel('Predicted probability')\n",
    "axes[0,0].set_ylabel('True probability')\n",
    "axes[1,0].set_ylabel('True probability')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "\n",
    "4.1) Explain what see you in the figures above and how calibration curves are calculated\n",
    "\n",
    "\n",
    "4.2) Why do we need a fairly large test set to study calibration?\n",
    "\n",
    "\n",
    "4.3) Are all 4 class probabilities equally well/poorly calibrated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sometimes improve calibration by applying a post-hoc monotonic transformation. This simplest version is called **Platt scaling**. The idea is that we fit a logistic regression model to the **binary targets** using the **predicted class probabilities** as inputs. That is, we want to fit the following model\n",
    "\n",
    "\\begin{align*}\n",
    "P(t_n=1|\\mathbf{x}_n) = \\frac{1}{1 + \\exp(A  + B\\cdot f(\\mathbf{x}_n))},\n",
    "\\end{align*}\n",
    "\n",
    "where $f(\\mathbf{x}_n)$ denotes the predicted probability for $\\mathbf{x}_n$. We fit the two scalar parameters $A$ and $B$ using maximum likelihood. \n",
    "\n",
    "That is, instead of using $f(\\mathbf{x})$ as predictive probability for $\\mathbf{x}^*$, we will use \n",
    "\n",
    "\\begin{align*}\n",
    "f_{\\text{calibrated}}(\\mathbf{x}^*) = \\frac{1}{1 + \\exp(\\hat{A}_{ML}  + \\hat{B}_{ML}\\cdot f(\\mathbf{x}^*))}\n",
    "\\end{align*}\n",
    "\n",
    "Below is a simple implementation of Platt scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1./(1+ np.exp(-x))\n",
    "\n",
    "class PlattCalibration(object):\n",
    "    \n",
    "    def __init__(self, t_binary, probs, num_bins=10):\n",
    "        \n",
    "        self.num_bins = num_bins\n",
    "        self.t_binary = t_binary\n",
    "        self.probs = probs\n",
    "        self.fit()\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        t_binary_scaled = np.array(self.t_binary)\n",
    "        Nplus = np.sum(t_binary_scaled==1)\n",
    "        Nminus = np.sum(t_binary_scaled==0)\n",
    "    \n",
    "        # John Platt advocates using smoothed target to avoid overfitting\n",
    "        t_binary_scaled[self.t_binary==1] = (Nplus + 1)/(Nplus + 2)\n",
    "        t_binary_scaled[self.t_binary==0] = 1/(Nminus + 2)\n",
    "    \n",
    "        # define log likelihood\n",
    "        log_lik = lambda w: t_binary_scaled*np.log(self.calibrate(self.probs, w)) + (1-t_binary_scaled)*np.log(1-self.calibrate(self.probs, w))\n",
    "    \n",
    "        # optimize the negative log likelihood from above\n",
    "        obj = lambda w: -np.sum(log_lik(w))\n",
    "        res = minimize(value_and_grad(obj), np.zeros(2), jac=True)\n",
    "        self.w = res.x\n",
    "        \n",
    "    def calibrate(self, p, w=None):\n",
    "        if w is None:\n",
    "            w = self.w\n",
    "            \n",
    "        return sigmoid(w[0] + w[1]*p)\n",
    "        \n",
    "        \n",
    "ps = np.linspace(0, 1, 100)\n",
    "                                \n",
    "fix, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "for class_idx in range(num_classes):\n",
    "    \n",
    "    ax = axes.flatten()[class_idx]\n",
    "    p_true_mean, p_true_se, p_pred = compute_calibration_curve(ttest==class_idx, p_test_bls[:, class_idx])\n",
    "    \n",
    "    # calibrate using Platt scaling\n",
    "    platt = PlattCalibration(1.0*(ttest==class_idx), p_test_bls[:, class_idx])        \n",
    "    p_pred_cal = platt.calibrate(p_pred)\n",
    "        \n",
    "    ax.errorbar(p_pred, p_true_mean, p_true_se, label=\"Raw\", color=colors[class_idx], marker='o', markersize=8)\n",
    "    ax.plot(ps, platt.calibrate(ps), color='m', label='Fitted function')\n",
    "    ax.plot(p_pred_cal, p_true_mean, label=\"Calibrated\", color=colors[class_idx], linestyle='-', alpha=0.5)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "axes[1,0].set_xlabel('Predicted probability')\n",
    "axes[1,1].set_xlabel('Predicted probability')\n",
    "axes[0,0].set_ylabel('True probability')\n",
    "axes[1,0].set_ylabel('True probability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "\n",
    "4.4) Explain how Platt scaling works in your own words.\n",
    "\n",
    "4.5) How well does Platt scaling work for each of the four classes?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml2023",
   "language": "python",
   "name": "bml2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
